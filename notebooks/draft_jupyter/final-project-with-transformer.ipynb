{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ***Импорт библиотек***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #лемматизация и стемминг уже внутри вшиты\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, DBSCAN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Создаем датасет с лайками пользователей и вытаскиваем фитчи из взаимодействий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создадим переменную connection_path для того чтобы подключаться к базе данных не указывая явно в коде логин и пароль от БД\n",
    "\n",
    "config_file = \"config.txt\"\n",
    "with open(config_file, \"r\") as f:\n",
    "    config_data = f.readlines()\n",
    "\n",
    "config = {}\n",
    "for line in config_data:\n",
    "    key, value = line.strip().split(\"=\")\n",
    "    config[key] = value\n",
    "    \n",
    "connection_path = f\"postgresql://{config['username']}:{config['password']}@{config['host']}:{config['port']}/{config['database']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_sql(query:str):\n",
    "    conn_uri = connection_path\n",
    "    df = pd.read_sql(query, conn_uri)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_to_sql(df:DataFrame, table_name: str):\n",
    "    \n",
    "    engine = create_engine(\n",
    "        connection_path\n",
    "    )\n",
    "\n",
    "    df.to_sql(table_name, con=engine, index=False, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_post_seen_and_liked_by_user = '''\n",
    "    SELECT\n",
    "        f.user_id,\n",
    "        p.topic,\n",
    "        COUNT(CASE WHEN f.action = 'view' THEN 1 ELSE NULL END) AS seen_posts,\n",
    "        COUNT(CASE WHEN f.action = 'like' THEN 1 ELSE NULL END) AS liked_posts\n",
    "    FROM\n",
    "        public.feed_data AS f\n",
    "    JOIN\n",
    "        public.post_text_df AS p ON f.post_id = p.post_id\n",
    "    GROUP BY\n",
    "        f.user_id, p.topic\n",
    "'''\n",
    "df_most_liked_posts_by_topic = get_data_from_sql(query_post_seen_and_liked_by_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Группировка данных по 'user_id' и выбор топ-3 для каждой группы\n",
    "top_3_per_user = df_most_liked_posts_by_topic.sort_values(by='liked_posts', ascending=False).groupby('user_id').head(3)\n",
    "# Создание новой колонки 'top_3' с темами\n",
    "df_most_liked_posts_by_topic['top_3'] = top_3_per_user.groupby('user_id')['topic'].agg(list).reindex(df_most_liked_posts_by_topic['user_id']).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_most_liked_posts_by_topic.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_likes_by_user = '''\n",
    "    SELECT p.user_id, ARRAY_AGG(DISTINCT p.post_id) AS liked_posts\n",
    "    FROM public.feed_data AS p\n",
    "    WHERE p.target = 1\n",
    "    GROUP BY p.user_id\n",
    "'''\n",
    "df_liked_posts = get_data_from_sql(query_likes_by_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_most_liked_posts_by_topic нет дубликатов по 'user_id'\n",
    "df_most_liked_posts_by_topic_ = df_most_liked_posts_by_topic.drop_duplicates(subset='user_id').copy()\n",
    "# выполните мердж\n",
    "df_liked_posts = pd.merge(df_liked_posts, df_most_liked_posts_by_topic_[['user_id', 'top_3']], on='user_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_liked_posts.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data_to_sql(df_liked_posts,'koryakovda_features_users_actions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Загрузка и обработка фитчей и выгрузка для таблицы user***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def users_average_age_per_city(df):\n",
    "    av_age = df_user_data.groupby('city')['age'].mean()\n",
    "    df['av_age_per_city'] = df['city'].map(av_age)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_users_in_country(df):\n",
    "    count = df['country'].value_counts()  # Count the number of users in each country\n",
    "    df['users_in_country'] = df['country'].map(count)  # Map the counts back to the original DataFrame\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_age_split(df):\n",
    "    bins = [0, 12, 17, 24, 34, 44, 54, 64, float('inf')]  # Определение границ бинов\n",
    "    labels = ['Дети','Подростки', 'Молодежь', 'Молодые взрослые', 'Взрослые', 'Средний возраст', 'Старшее поколение', 'Пожилые']  # Названия групп\n",
    "    df['age_group'] = pd.cut(df['age'], bins=bins, labels=labels, right=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_feature_creation(df):\n",
    "    df = user_age_split(df)\n",
    "    df = count_users_in_country(df)\n",
    "    df = users_average_age_per_city(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_data = get_data_from_sql(\"SELECT * FROM public.user_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_user_data = user_feature_creation(df_user_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_data_to_sql(df_user_data,'koryakovda_features_users')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Загрузка и обработка фитчей и выгрузка для таблицы posts с использованием transformer (векторизуем тексты не через tfidf, а через distilbert)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post_text_for_nn = get_data_from_sql(\"SELECT * FROM public.post_text_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post_text_for_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Сделаем эмбеддинги постов с помощью моделей трансформеров\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import BertModel  # https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertModel\n",
    "from transformers import RobertaModel  # https://huggingface.co/docs/transformers/model_doc/roberta#transformers.RobertaModel\n",
    "from transformers import DistilBertModel  # https://huggingface.co/docs/transformers/model_doc/distilbert#transformers.DistilBertModel\n",
    "\n",
    "\n",
    "def get_model(model_name):\n",
    "    assert model_name in ['bert', 'roberta', 'distilbert']\n",
    "\n",
    "    checkpoint_names = {\n",
    "        'bert': 'bert-base-cased',  # https://huggingface.co/bert-base-cased\n",
    "        'roberta': 'roberta-base',  # https://huggingface.co/roberta-base\n",
    "        'distilbert': 'distilbert-base-cased'  # https://huggingface.co/distilbert-base-cased\n",
    "    }\n",
    "\n",
    "    model_classes = {\n",
    "        'bert': BertModel,\n",
    "        'roberta': RobertaModel,\n",
    "        'distilbert': DistilBertModel\n",
    "    }\n",
    "\n",
    "    return AutoTokenizer.from_pretrained(checkpoint_names[model_name]), model_classes[model_name].from_pretrained(checkpoint_names[model_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, model = get_model('distilbert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Сделаем датасет для постов\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "\n",
    "class PostDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer):\n",
    "        super().__init__()\n",
    "\n",
    "        self.texts = tokenizer.batch_encode_plus(\n",
    "            texts,\n",
    "            add_special_tokens=True,\n",
    "            return_token_type_ids=False,\n",
    "            return_tensors='pt',\n",
    "            truncation=True,\n",
    "            padding=True\n",
    "        )\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'input_ids': self.texts['input_ids'][idx], 'attention_mask': self.texts['attention_mask'][idx]}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts['input_ids'])\n",
    "    \n",
    "    \n",
    "dataset = PostDataset(df_post_text_for_nn['text'].values.tolist(), tokenizer)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=32, collate_fn=data_collator, pin_memory=True, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def get_embeddings_labels(model, loader):\n",
    "    model.eval()\n",
    "    \n",
    "    total_embeddings = []\n",
    "    \n",
    "    for batch in tqdm(loader):\n",
    "        batch = {key: batch[key].to(device) for key in ['attention_mask', 'input_ids']}\n",
    "\n",
    "        embeddings = model(**batch)['last_hidden_state'][:, 0, :]\n",
    "\n",
    "        total_embeddings.append(embeddings.cpu())\n",
    "\n",
    "    return torch.cat(total_embeddings, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "print(torch.cuda.get_device_name())\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = get_embeddings_labels(model, loader).numpy()\n",
    "\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Пытаемся кластеризовать тексты как и делали до этого\n",
    "\n",
    "centered = embeddings - embeddings.mean()\n",
    "\n",
    "pca = PCA(n_components=50)\n",
    "pca_decomp = pca.fit_transform(centered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post_text_for_nn.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 16\n",
    "\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(pca_decomp)\n",
    "\n",
    "df_post_text_for_nn['kmeans_labels'] = kmeans.labels_\n",
    "\n",
    "dists_columns = [f'distance_to_cluster_{i}' for i in range(n_clusters)]\n",
    "\n",
    "dists_df = pd.DataFrame(\n",
    "    data=kmeans.transform(pca_decomp),\n",
    "    columns=dists_columns\n",
    ")\n",
    "\n",
    "dists_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post_text_for_nn = pd.concat((df_post_text_for_nn, dists_df), axis=1)\n",
    "\n",
    "df_post_text_for_nn.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data_to_sql(df_post_text_for_nn, 'koryakovda_features_post_transformer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ***Обучение модели***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_load_sql(query: str) -> pd.DataFrame:\n",
    "    CHUNKSIZE = 200000\n",
    "    engine = create_engine(connection_path)\n",
    "    conn = engine.connect().execution_options(stream_results=True)\n",
    "    chunks = []\n",
    "    for chunk_dataframe in pd.read_sql(query, conn, chunksize=CHUNKSIZE):\n",
    "        chunks.append(chunk_dataframe)\n",
    "    conn.close()\n",
    "    return pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "def load_features() -> pd.DataFrame:\n",
    "    query = 'SELECT * FROM public.feed_data LIMIT 2000000'\n",
    "    return batch_load_sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Выгружаем табличку взаимодействий (размер корректируем для обучения)\n",
    "user_post_iteractions = load_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_post_iteractions.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выгружаем обработанные таблички из SQL\n",
    "features_users_df = get_data_from_sql(\"SELECT * FROM koryakovda_features_users\")\n",
    "features_post_df_transformer = get_data_from_sql(\"SELECT * FROM koryakovda_features_post_transformer\")\n",
    "features_users_actions_df = get_data_from_sql(\"SELECT * FROM koryakovda_features_users_actions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Обьединяем датасеты в один большой для исследования\n",
    "def create_dataset(user_post_iteractions, features_users_df, features_post_df_transformer, features_users_actions_df):\n",
    "    step_one = pd.merge(user_post_iteractions, features_users_df, on='user_id')\n",
    "    step_two = pd.merge(step_one, features_post_df_transformer, on='post_id')\n",
    "    step_three = pd.merge(step_two, features_users_actions_df[['user_id', 'top_3']], on='user_id')\n",
    "\n",
    "    return step_three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = create_dataset(user_post_iteractions,features_users_df,features_post_df_transformer,features_users_actions_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Добавление новых фитчей в итоговый датасет на основе временных данных***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выделение часа и дня недели  - предположение что в зависимости от дня недели и часа дня утро\\день\\вечер - человека интересуюр разные потсы\n",
    "def create_time_features(df):\n",
    "    df['hour'] = df['timestamp'].dt.hour\n",
    "    df['day_of_week'] = df['timestamp'].dt.day_name()\n",
    "    df['month'] = df['timestamp'].dt.month\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = create_time_features(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Построим графики нашего предположения - изменение отношения показов поста к количеству лайков от дня недели и часа дня***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_show_like_ratio_by_day(df, days_order=None):\n",
    "    # Группировка данных по теме, дню недели и подсчет количества показанных и лайкнутых постов\n",
    "    grouped_data = df.groupby(['topic', 'day_of_week', 'action']).size().reset_index(name='count')\n",
    "\n",
    "    days_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "    # Указание порядка дней недели\n",
    "    grouped_data['day_of_week'] = pd.Categorical(grouped_data['day_of_week'], categories=days_order, ordered=True)\n",
    "\n",
    "    # Создание сводной таблицы с количеством показанных и лайкнутых постов по темам и дням недели\n",
    "    pivot_table = pd.pivot_table(grouped_data, values='count', index=['topic', 'day_of_week'], columns='action', fill_value=0)\n",
    "\n",
    "    # Вычисление отношения количества показанных постов к количеству лайкнутых постов\n",
    "    pivot_table['view_like_ratio'] = pivot_table['view'] / pivot_table['like']\n",
    "\n",
    "    # Построение графика\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.lineplot(x='day_of_week', y='view_like_ratio', hue='topic', data=pivot_table.reset_index(), marker='o')\n",
    "    plt.title('Show-to-Like Ratio by Topic and Day of Week')\n",
    "    plt.xlabel('Day of Week')\n",
    "    plt.ylabel('Show-to-Like Ratio')\n",
    "    plt.legend(title='Topic', loc='upper right')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.show()\n",
    "\n",
    "# Пример использования функции с вашим датасетом\n",
    "\n",
    "plot_show_like_ratio_by_day(dataset, days_order=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_show_like_ratio_by_hour(df):\n",
    "    # Группировка данных по теме, часу дня и подсчет количества показанных и лайкнутых постов\n",
    "    grouped_data = df.groupby(['topic', 'hour', 'action']).size().reset_index(name='count')\n",
    "\n",
    "    # Создание сводной таблицы с количеством показанных и лайкнутых постов по темам и часам дня\n",
    "    pivot_table = pd.pivot_table(grouped_data, values='count', index=['topic', 'hour'], columns='action', fill_value=0)\n",
    "\n",
    "    # Вычисление отношения количества показанных постов к количеству лайкнутых постов\n",
    "    pivot_table['view_like_ratio'] = pivot_table['view'] / pivot_table['like']\n",
    "\n",
    "    # Построение графика\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.lineplot(x='hour', y='view_like_ratio', hue='topic', data=pivot_table.reset_index(), marker='o')\n",
    "    plt.title('Show-to-Like Ratio by Topic and Hour of Day')\n",
    "    plt.xlabel('Hour of Day')\n",
    "    plt.ylabel('Show-to-Like Ratio')\n",
    "    plt.legend(title='Topic', loc='upper right')\n",
    "    plt.xticks(grouped_data['hour'].unique())  \n",
    "    plt.show()\n",
    "\n",
    "plot_show_like_ratio_by_hour(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "dataset.head(2)\n",
    "# pd.reset_option('display.max_columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Подготовка к построению модели***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Модель будем строить по столбцу target поэтому сразу удаляем из датасета взаимодействия-повторы где like target = 0 (там view = 1)\n",
    "dataset = dataset[dataset['action'] != 'like']\n",
    "dataset.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сортируем датасет для дальнейшего деления на трейн и тест по timestamp\n",
    "dataset = dataset.sort_values(by = 'timestamp')\n",
    "dataset.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выделяем категориальный фитчи\n",
    "cat_features = ['gender','country','city','exp_group','os','source','age_group','topic','kmeans_labels','day_of_week','hour', 'month', 'top_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Делим на трейн и тест выделив для трейна 80% и теста 20% выборки\n",
    "num_rows = int(len(dataset) * 0.2)\n",
    "train_set = dataset.iloc[:-num_rows]\n",
    "test_set = dataset.iloc[-num_rows:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Создадим список для каждого юзера с постами которые были лайкнуты на трейне чтобы удалить их на тесте чтобы модель их не рекомендовала. \n",
    "# т.к. рекоменовать человеку снова что-то уже лайкнутое в трейне как будто не логично (подглядываем в ответ)\n",
    "liked_posts_by_users_train = train_set[train_set['target'] == 1][['user_id', 'post_id']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Уберем из теста лайкнутые в трейне посты\n",
    "merged_data = test_set.merge(liked_posts_by_users_train, on=['user_id', 'post_id'], how='left', indicator=True)\n",
    "filtered_test_set = merged_data[merged_data['_merge'] == 'left_only']\n",
    "filtered_test_set = filtered_test_set.drop(columns=['_merge'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_test_set.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Работа по предсказанию***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# поделим данные на фитчи и таргет\n",
    "X_train = train_set.drop('target', axis=1)\n",
    "X_test = filtered_test_set.drop('target', axis=1)\n",
    "\n",
    "y_train = train_set['target']\n",
    "y_test = filtered_test_set['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удаляем ненужные стлобцы, создаем новый датасет для работы\n",
    "X_train_cut = X_train.drop(['timestamp','text','user_id','post_id','action'],axis =1)\n",
    "X_test_cut = X_test.drop(['timestamp','text','user_id','post_id','action'],axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cut.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Обучаем Catboost\n",
    "catboost = CatBoostClassifier()\n",
    "catboost.fit(X_train_cut,y_train, cat_features=cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Считаем предсказание\n",
    "X_test_cut['content_prediction'] = catboost.predict_proba(X_test_cut)[:, 1]\n",
    "\n",
    "# Добавляем истиный target, user_id, post_id для дальнейшего подсчета метрики\n",
    "X_test_cut['target'] = y_test\n",
    "X_test_cut['user_id'] = X_test['user_id']\n",
    "X_test_cut['post_id'] = X_test['post_id']\n",
    "\n",
    "X_test_cut.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Оцениваем наш HitRate@5*** \\\n",
    "Имеет ли смысл оценка на тесте если в тестовой выборке у нас не все посты для каждого пользователя а только малая часть?  \n",
    "Как будто нет, но все равно сделаем для консистенции, реально тестить модель будем уже в сервисе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hitrate_at_5(X_test_cut, limit):\n",
    "    hits = []\n",
    "    for user in X_test_cut['user_id'].unique():\n",
    "        part = X_test_cut[X_test_cut['user_id']== user]\n",
    "        part = part.sort_values('content_prediction',ascending=False)[:limit] # выбираем топ N рекомендаций\n",
    "        if part['target'].sum() >= 1:\n",
    "            hit = 1\n",
    "        else:\n",
    "            hit = 0\n",
    "        hits.append(hit)\n",
    "    hitrate_at_5 = sum(hits) / len(X_test_cut['user_id'].unique())\n",
    "    \n",
    "    return hitrate_at_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оцениваем качество модели\n",
    "hitrate_at_5(X_test_cut,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Сохранение / загрузка модели***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost.save_model('model_nn', format=\"cbm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_model_path(path: str) -> str:\n",
    "#     if os.environ.get(\"IS_LMS\") == \"1\":  # проверяем где выполняется код в лмс, или локально.\n",
    "#         MODEL_PATH = '/workdir/user_input/model'\n",
    "#     else:\n",
    "#         MODEL_PATH = path\n",
    "#     return MODEL_PATH\n",
    "\n",
    "# def load_models():\n",
    "#     model_path = get_model_path(\"./catboost_model\")  # Предпложим что данные в той же директории\n",
    "#     from_file = CatBoostClassifier()\n",
    "#     model = from_file.load_model(model_path)\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_path(model_version: str) -> str:\n",
    "    \"\"\"\n",
    "    Здесь мы модицифируем функцию так, чтобы иметь возможность загружать\n",
    "    обе модели. При этом мы могли бы загружать и приципиально разные\n",
    "    модели, так как никак не ограничены тем, какой код использовать.\n",
    "    \"\"\"\n",
    "    print(os.environ)\n",
    "    if (\n",
    "        os.environ.get(\"IS_LMS\") == \"1\"\n",
    "    ):  # проверяем где выполняется код в лмс, или локально. Немного магии\n",
    "        model_path = f\"/workdir/user_input/{model_version}\"\n",
    "    else:\n",
    "        model_path = (f\"./{model_version}\")\n",
    "    return model_path\n",
    "\n",
    "\n",
    "def load_models(model_version: str):\n",
    "    model_path = get_model_path(model_version)\n",
    "    loaded_model = CatBoostClassifier()\n",
    "    loaded_model.load_model(model_path)\n",
    "    return loaded_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Проверка работы на примере конкретного юзера. Формирование сервиса - полного цикла предсказаний***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Берем любого юзера с фитчами прикручиваем к каждому посту и убираем лайкнутые посты чтобы их не рекомендовать "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выгружаем обработанные таблички из SQL\n",
    "features_users_df = get_data_from_sql(\"SELECT * FROM koryakovda_features_users\")\n",
    "features_post_df_transformer = get_data_from_sql(\"SELECT * FROM koryakovda_features_post_transformer\")\n",
    "features_users_actions_df = get_data_from_sql(\"SELECT * FROM koryakovda_features_users_actions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_users_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_post_df_transformer.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_users_actions_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для примера возьмем user_id = 202\n",
    "user_features = features_users_df[features_users_df['user_id'] == 202].copy()\n",
    "user_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Обрабатываем список всех постов которые лайкнул пользователь, чтобы не рекоендовать их потом\n",
    "user_likes = features_users_actions_df[features_users_actions_df['user_id']==202]['liked_posts'].iloc[0]\n",
    "user_likes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_likes = user_likes.strip('{}').split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_likes = [int(x) for x in user_likes]\n",
    "print(user_likes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удаляем из датасета со всеми обработанными постами посты уже лайкнутые пользователем чтобы не рекомендовать их снова\n",
    "posts_features = features_post_df_transformer[~features_post_df_transformer['post_id'].isin(user_likes)].copy()\n",
    "posts_features.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Задаем время рекомендации как входной параметр\n",
    "time = datetime(year=2021, month=1, day=3, hour=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# добавляем временные фитчи в финальный датасет\n",
    "posts_features['hour'] = time.hour\n",
    "posts_features['day_of_week'] = time.strftime(\"%A\")\n",
    "posts_features['month'] = time.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# мерджим выбранного в запросе юзера к каждому посту\n",
    "final_df = posts_features.assign(**user_features.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Присоединим top3 категории к датасету\n",
    "final_df = pd.merge(final_df,features_users_actions_df[['user_id','top_3']], on = 'user_id', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# удаляем из финального датасета ненужные столбцы\n",
    "final_df = final_df.drop(['text', 'user_id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# задаем post_id как индекс нашего финального датасета\n",
    "final_df.set_index('post_id',inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Приводим трейн датасет и рабочий в соответствие по порядку строк\n",
    "desired_column_order = X_train_cut.columns.tolist()\n",
    "final_df = final_df[desired_column_order]\n",
    "print(desired_column_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузим обученную ранее модель с использованием функции из степпа 2\n",
    "model = load_models('model_nn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# считаем предсказания\n",
    "preds = model.predict_proba(final_df)[:, 1]\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# выбираем топ 5 постов по предсказаниям вероятности лайка\n",
    "preds = pd.DataFrame(model.predict_proba(final_df)[:, 1], columns=['probability'], index=final_df.index)\n",
    "top5_predictions = preds.nlargest(5, 'probability').index\n",
    "top5_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузим класс для типизации выходного ответа\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class PostGet(BaseModel):\n",
    "    id: int\n",
    "    text: str\n",
    "    topic: str\n",
    "\n",
    "    class Config:\n",
    "        orm_mode = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выдаем 5 постов рекомендаций по юзеру\n",
    "recommended_posts = []\n",
    "for post_id in top5_predictions:\n",
    "    post_data = features_post_df_transformer.loc[features_post_df_transformer['post_id'] == post_id]\n",
    "    if not post_data.empty:\n",
    "        post = PostGet(\n",
    "            id=post_id,\n",
    "            text=post_data['text'].iloc[0],\n",
    "            topic=post_data['topic'].iloc[0]\n",
    "        )\n",
    "        recommended_posts.append(post)\n",
    "\n",
    "recommended_posts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Итоговый сервис:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Так должн выглядеть мой эндпоинт для предсказания 5 постов по пользователю\n",
    "import os\n",
    "from typing import List\n",
    "from fastapi import FastAPI\n",
    "from datetime import datetime\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# ***Загрузка модели CatBoostClassifier***\n",
    "model = load_models('model_nn')\n",
    "# передадим порядок колонок из трейна, чтобы далее датасет для предсказаний сделать таким же\n",
    "desired_column_order = ['gender','age','country','city','exp_group','os','source','age_group','users_in_country','av_age_per_city',\n",
    "                        'topic','word_count','tfidf_sum','tfidf_mean','tfidf_max','kmeans_labels','distance_to_cluster_0',\n",
    "                        'distance_to_cluster_1','distance_to_cluster_2','distance_to_cluster_3','distance_to_cluster_4','distance_to_cluster_5',\n",
    "                        'distance_to_cluster_6','distance_to_cluster_7','distance_to_cluster_8','distance_to_cluster_9','distance_to_cluster_10',\n",
    "                        'distance_to_cluster_11','distance_to_cluster_12','distance_to_cluster_13','distance_to_cluster_14','distance_to_cluster_15',\n",
    "                        'top_3','hour','day_of_week']\n",
    "\n",
    "# ***Загрузка обработанных фитчей для таблицы user***\n",
    "# df_user_data = get_data_from_sql(\"SELECT * FROM koryakovda_features_users\")\n",
    "features_users_df = get_data_from_sql(\"SELECT * FROM koryakovda_features_users\")\n",
    "\n",
    "# ***Загрузка обработанных фитчей для таблицы posts***\n",
    "# df_posts = get_data_from_sql(\"SELECT * FROM koryakovda_features_post\")\n",
    "features_post_df_transformer = get_data_from_sql(\"SELECT * FROM koryakovda_features_post_transformer\")\n",
    "\n",
    "# ***Загрузка датасет с лайками пользователей и фитчами из взаимодействий***\n",
    "# df_liked_posts = get_data_from_sql(\"SELECT * FROM koryakovda_features_users_actions\")\n",
    "features_users_actions_df = get_data_from_sql(\"SELECT * FROM koryakovda_features_users_actions\")\n",
    "\n",
    "\n",
    "\n",
    "@app.get(\"/post/recommendations/\", response_model=List[PostGet])\n",
    "def get_post_recommendations(id: int, time: datetime, limit: int = 5):\n",
    "    \n",
    "\n",
    "    user_features = features_users_df[features_users_df['user_id'] == id].copy()\n",
    "    user_likes = features_users_actions_df[features_users_actions_df['user_id'] == id]['liked_posts'].iloc[0]\n",
    "    user_likes = user_likes.strip('{}').split(',')\n",
    "    user_likes = [int(x) for x in user_likes]\n",
    "    posts_features = features_post_df_transformer[~features_post_df_transformer['post_id'].isin(user_likes)].copy()\n",
    "    posts_features['hour'] = time.hour\n",
    "    posts_features['day_of_week'] = time.strftime(\"%A\")\n",
    "    posts_features['month'] = time.month\n",
    "    final_df = posts_features.assign(**user_features.iloc[0])\n",
    "    final_df = pd.merge(final_df, features_users_actions_df[['user_id', 'top_3']], on='user_id', how='left')\n",
    "    final_df = final_df.drop(['text', 'user_id'], axis=1)\n",
    "    final_df.set_index('post_id', inplace=True)\n",
    "    final_df = final_df[desired_column_order]\n",
    "    preds = pd.DataFrame(model.predict_proba(final_df)[:, 1], columns=['probability'], index=final_df.index)\n",
    "    top5_predictions = preds.nlargest(5, 'probability').index\n",
    "    \n",
    "    recommended_posts = []\n",
    "    for post_id in top5_predictions:\n",
    "        post_data = features_post_df_transformer.loc[features_post_df_transformer['post_id'] == post_id]\n",
    "        if not post_data.empty:\n",
    "            post = PostGet(\n",
    "                id=post_id,\n",
    "                text=post_data['text'].iloc[0],\n",
    "                topic=post_data['topic'].iloc[0]\n",
    "            )\n",
    "            recommended_posts.append(post)\n",
    "\n",
    "    return recommended_posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Весь сервис app.py для того чтобы протестить обе модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "SQLALCHEMY_DATABASE_URL = connection_path\n",
    "\n",
    "engine = create_engine(SQLALCHEMY_DATABASE_URL)\n",
    "SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n",
    "\n",
    "Base = declarative_base()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class UserGet(BaseModel):\n",
    "    id: int\n",
    "    gender: int\n",
    "    age: int\n",
    "    country: str\n",
    "    city: str\n",
    "    exp_group: int\n",
    "    os: str\n",
    "    source: str\n",
    "\n",
    "    class Config:\n",
    "        orm_mode = True\n",
    "\n",
    "\n",
    "class PostGet(BaseModel):\n",
    "    id: int\n",
    "    text: str\n",
    "    topic: str\n",
    "\n",
    "    class Config:\n",
    "        orm_mode = True\n",
    "\n",
    "class Response(BaseModel):\n",
    "    exp_group: str\n",
    "    recommendations: List[PostGet]\n",
    "\n",
    "class FeedGet(BaseModel):\n",
    "    user_id: int\n",
    "    post_id: int\n",
    "    action: str\n",
    "    time: datetime.datetime\n",
    "    user: UserGet\n",
    "    post: PostGet\n",
    "\n",
    "    class Config:\n",
    "        orm_mode = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException, Depends\n",
    "from sqlalchemy.orm import Session\n",
    "from sqlalchemy import desc, func, create_engine\n",
    "from typing import List\n",
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from catboost import CatBoostClassifier\n",
    "import hashlib\n",
    "\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "\n",
    "def get_db():\n",
    "    with SessionLocal() as db:\n",
    "        return db\n",
    "\n",
    "def get_model_path(model_version: str) -> str:\n",
    "    \"\"\"\n",
    "    Здесь мы модицифируем функцию так, чтобы иметь возможность загружать\n",
    "    обе модели. При этом мы могли бы загружать и приципиально разные\n",
    "    модели, так как никак не ограничены тем, какой код использовать.\n",
    "    \"\"\"\n",
    "    print(os.environ)\n",
    "    if (\n",
    "        os.environ.get(\"IS_LMS\") == \"1\"\n",
    "    ):  # проверяем где выполняется код в лмс, или локально. Немного магии\n",
    "        model_path = f\"/workdir/user_input/{model_version}\"\n",
    "    else:\n",
    "        # Пробуем загрузить модель из текущей директории (если такой модели не будет будет просто присвоено None)\n",
    "        model_path = f\"./{model_version}\"\n",
    "    return model_path\n",
    "\n",
    "def load_models(model_version: str):\n",
    "    model_path = get_model_path(model_version)\n",
    "    loaded_model = CatBoostClassifier()\n",
    "    loaded_model.load_model(model_path)\n",
    "    return loaded_model\n",
    "\n",
    "# ***Загрузка и обработка фитчей и выгрузка для таблицы user***\n",
    "def get_data_from_sql(query: str):\n",
    "    conn_uri = connection_path\n",
    "    df = pd.read_sql(query, conn_uri)\n",
    "    return df\n",
    "\n",
    "SALT = \"my_salt\"\n",
    "\n",
    "def get_exp_group(user_id: int) -> str:\n",
    "    value_str = str(id) + SALT\n",
    "    value_num = int(hashlib.md5(value_str.encode()).hexdigest(), 16)\n",
    "    percent = value_num % 100\n",
    "    if percent < 50:\n",
    "        return \"control\"\n",
    "    elif percent < 100:\n",
    "        return \"test\"\n",
    "    return \"unknown\"\n",
    "\n",
    "# ***Загрузка моделей CatBoostClassifier***\n",
    "model_control = load_models('model_tfidf')\n",
    "model_test = load_models('model_nn')\n",
    "\n",
    "# передадим порядок колонок из трейна, чтобы далее датасет для предсказаний сделать таким же\n",
    "desired_column_order = ['gender','age','country','city','exp_group','os','source','age_group','users_in_country','av_age_per_city',\n",
    "                        'topic','word_count','tfidf_sum','tfidf_mean','tfidf_max','kmeans_labels','distance_to_cluster_0',\n",
    "                        'distance_to_cluster_1','distance_to_cluster_2','distance_to_cluster_3','distance_to_cluster_4','distance_to_cluster_5',\n",
    "                        'distance_to_cluster_6','distance_to_cluster_7','distance_to_cluster_8','distance_to_cluster_9','distance_to_cluster_10',\n",
    "                        'distance_to_cluster_11','distance_to_cluster_12','distance_to_cluster_13','distance_to_cluster_14','distance_to_cluster_15',\n",
    "                        'top_3','hour','day_of_week']\n",
    "\n",
    "# ***Загрузка обработанных фитчей для таблицы user***\n",
    "# df_user_data = get_data_from_sql(\"SELECT * FROM koryakovda_features_users\")\n",
    "features_users_df = get_data_from_sql(\"SELECT * FROM koryakovda_features_users\")\n",
    "\n",
    "# ***Загрузка обработанных фитчей для таблицы posts***\n",
    "# df_posts = get_data_from_sql(\"SELECT * FROM koryakovda_features_post\")\n",
    "features_post_df_transformer = get_data_from_sql(\"SELECT * FROM koryakovda_features_post_transformer\")\n",
    "\n",
    "# ***Загрузка обработанных фитчей для таблицы posts***\n",
    "# df_posts = get_data_from_sql(\"SELECT * FROM koryakovda_features_post\")\n",
    "features_post_df = get_data_from_sql(\"SELECT * FROM koryakovda_features_post\")\n",
    "\n",
    "# ***Загрузка датасет с лайками пользователей и фитчами из взаимодействий***\n",
    "# df_liked_posts = get_data_from_sql(\"SELECT * FROM koryakovda_features_users_actions\")\n",
    "features_users_actions_df = get_data_from_sql(\"SELECT * FROM koryakovda_features_users_actions\")\n",
    "\n",
    "\n",
    "# ***FastAPI***\n",
    "@app.get(\"/user/{id}\", response_model=UserGet)\n",
    "def get_user_id(id: int, db: Session = Depends(get_db)):\n",
    "    result = db.query(User).filter(User.id == id).first()\n",
    "    if result is None:\n",
    "        raise HTTPException(404, 'id not found')\n",
    "    else:\n",
    "        return result\n",
    "\n",
    "\n",
    "@app.get(\"/post/{id}\", response_model=PostGet)\n",
    "def get_post_id(id: int, db: Session = Depends(get_db)):\n",
    "    result = db.query(Post).filter(Post.id == id).first()\n",
    "    if result is None:\n",
    "        raise HTTPException(404, 'id not found')\n",
    "    else:\n",
    "        return result\n",
    "\n",
    "\n",
    "@app.get(\"/user/{id}/feed\", response_model=List[FeedGet])\n",
    "def user_feed(id: int, limit: int = 10, db: Session = Depends(get_db)):\n",
    "    result = db.query(Feed).filter(Feed.user_id == id).order_by(desc(Feed.time)).limit(limit).all()\n",
    "    return result\n",
    "\n",
    "\n",
    "@app.get(\"/post/{id}/feed\", response_model=List[FeedGet])\n",
    "def post_feed(id: int, limit: int = 10, db: Session = Depends(get_db)):\n",
    "    result = db.query(Feed).filter(Feed.post_id == id).order_by(desc(Feed.time)).limit(limit).all()\n",
    "    return result\n",
    "\n",
    "@app.get(\"/post/recommendations/\", response_model=Response)\n",
    "def get_post_recommendations(id: int, time: datetime, limit: int = 5):\n",
    "    # Выбираем группу пользователи\n",
    "    user_group = get_exp_group(id=id)\n",
    "\n",
    "    # Выбираем нужную модель\n",
    "    if user_group == \"control\":\n",
    "        model = model_control\n",
    "        posts_features = features_post_df[~features_post_df['post_id'].isin(user_likes)].copy()\n",
    "    elif user_group == \"test\":\n",
    "        model = model_test\n",
    "        posts_features = features_post_df_transformer[~features_post_df_transformer['post_id'].isin(user_likes)].copy()\n",
    "    else:\n",
    "        raise ValueError(\"unknown group\")\n",
    "\n",
    "    user_features = features_users_df[features_users_df['user_id'] == id].copy()\n",
    "    user_likes = features_users_actions_df[features_users_actions_df['user_id'] == id]['liked_posts'].iloc[0]\n",
    "    user_likes = user_likes.strip('{}').split(',')\n",
    "    user_likes = [int(x) for x in user_likes]\n",
    "    posts_features['hour'] = time.hour\n",
    "    posts_features['day_of_week'] = time.strftime(\"%A\")\n",
    "    posts_features['month'] = time.month\n",
    "    final_df = posts_features.assign(**user_features.iloc[0])\n",
    "    final_df = pd.merge(final_df, features_users_actions_df[['user_id', 'top_3']], on='user_id', how='left')\n",
    "    final_df = final_df.drop(['text', 'user_id'], axis=1)\n",
    "    final_df.set_index('post_id', inplace=True)\n",
    "    final_df = final_df[desired_column_order]\n",
    "    preds = pd.DataFrame(model.predict_proba(final_df)[:, 1], columns=['probability'], index=final_df.index)\n",
    "    top5_predictions = preds.nlargest(5, 'probability').index\n",
    "\n",
    "    recommended_posts = []\n",
    "    for post_id in top5_predictions:\n",
    "        if user_group == \"control\":\n",
    "            post_data = features_post_df.loc[features_post_df['post_id'] == post_id]\n",
    "        elif user_group == \"test\":\n",
    "            post_data = features_post_df_transformer.loc[features_post_df_transformer['post_id'] == post_id]\n",
    "\n",
    "        if not post_data.empty:\n",
    "            post = PostGet(\n",
    "                id=post_id,\n",
    "                text=post_data['text'].iloc[0],\n",
    "                topic=post_data['topic'].iloc[0]\n",
    "            )\n",
    "            recommended_posts.append(post)\n",
    "\n",
    "    return Response(recommendations=recommended_posts, exp_group=user_group)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30665,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
